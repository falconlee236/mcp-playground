{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0260d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "\n",
    "from typing import Optional\n",
    "from contextlib import AsyncExitStack\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5316e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPClient:\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.session: Optional[ClientSession] = None\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.openai = OpenAI()\n",
    "    \n",
    "    async def connect_to_server(self, server_script_path: str):\n",
    "        \"\"\"Connect to an MCP server\n",
    "\n",
    "        Args:\n",
    "            server_script_path: Path to the server script (.py or .js)\n",
    "        \"\"\"\n",
    "        is_python = server_script_path.endswith('.py')\n",
    "        is_js = server_script_path.endswith('.js')\n",
    "        if not (is_python or is_js):\n",
    "            raise ValueError(\"Server script must be a .py or .js file\")\n",
    "        \n",
    "        command = \"python\" if is_python else \"node\"\n",
    "        server_params = StdioServerParameters(\n",
    "            command=command,\n",
    "            args=[server_script_path],\n",
    "            env=None\n",
    "        )\n",
    "        \n",
    "        # in python, context is critical section like OS that manages System resource safely \n",
    "        # the context manager code implemented python function is good to dynamic resource management\n",
    "        # similar to socket generation\n",
    "        # like generating socket\n",
    "        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n",
    "        # get write stream, read stream\n",
    "        self.stdio, self.write = stdio_transport\n",
    "        # get high level session with streams\n",
    "        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n",
    "        \n",
    "        await self.session.initialize()\n",
    "        \n",
    "        # List available tools\n",
    "        response = await self.session.list_tools()\n",
    "        tools = response.tools\n",
    "        print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
    "        \n",
    "    async def process_query(self, query: str) -> str:\n",
    "        \"\"\"Process a query using Claude and available tools\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        response = await self.session.list_tools()\n",
    "        available_tools = [{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.inputSchema,\n",
    "            }\n",
    "        } for tool in response.tools]\n",
    "        \n",
    "        # Initial OpenAI API call\n",
    "        response = self.openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            tools=available_tools,\n",
    "            tool_choice=\"auto\",\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        \n",
    "        # Process response and handle tool calls\n",
    "        final_text = []\n",
    "        assistant_message_content = []\n",
    "        message = response.choices[0].message\n",
    "        \n",
    "        # handling text response\n",
    "        if message.content:\n",
    "            final_text.append(message.content)\n",
    "            assistant_message_content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": message.content\n",
    "            })\n",
    "        # handling tool calls\n",
    "        elif message.tool_calls:\n",
    "            for tool_call in message.tool_calls:\n",
    "                tool_name = tool_call.function.name\n",
    "                # openai는 string 형식으로 들어온다\n",
    "                tool_args = json.loads(tool_call.function.arguments)\n",
    "                tool_call_id = tool_call.id\n",
    "                \n",
    "                # excute tool call\n",
    "                result = await self.session.call_tool(\n",
    "                    tool_name, \n",
    "                    tool_args,\n",
    "                )\n",
    "                final_text.append(f\"[Calling tool {tool_name} with args {tool_args}]\")\n",
    "                # append content\n",
    "                assistant_message_content.append({\n",
    "                    \"type\": \"function\",\n",
    "                    \"id\": tool_call_id,\n",
    "                    \"function\": {\n",
    "                        \"name\": tool_name,\n",
    "                        \"arguments\": json.dumps(tool_args), \n",
    "                    },\n",
    "                })\n",
    "                \n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"tool_calls\": assistant_message_content,\n",
    "                })\n",
    "                \n",
    "                # add tool response messages\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call_id,\n",
    "                    \"content\": result.content,\n",
    "                })\n",
    "                \n",
    "                # get next response from chat gpt\n",
    "                new_response = self.openai.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=messages,\n",
    "                    tools=available_tools,\n",
    "                    tool_choice=\"auto\",\n",
    "                    max_tokens=1000,\n",
    "                )\n",
    "                \n",
    "                # append next message text\n",
    "                new_message = new_response.choices[0].message\n",
    "                if new_message.content:\n",
    "                    final_text.append(new_message.content)\n",
    "        \n",
    "        return \"\\n\".join(final_text)\n",
    "    \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Client Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\n Query: \").strip()\n",
    "                \n",
    "                if query.lower() == \"quit\":\n",
    "                    break\n",
    "                \n",
    "                response = await self.process_query(query)\n",
    "                print(\"\\n\" + response)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "    \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "        \n",
    "async def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python client.py <path_to_server_script>\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    client = MCPClient()\n",
    "    try:\n",
    "        await client.connect_to_server(sys.argv[1])\n",
    "        await client.chat_loop()\n",
    "    finally:\n",
    "        await client.cleanup()\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    asyncio.run(main())\n",
    "    # What are the weather alerts in California"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
